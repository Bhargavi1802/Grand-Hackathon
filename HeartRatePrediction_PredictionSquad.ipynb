{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set up and environments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The code below is split between a jupyter notebook and .py script (Aider.py) ,which would be imported below for data preprocessing ,scaling,train test split ,running the model algoritms and generating the output with required metrics by returning a model object. \n",
    "\n",
    "2. The jupyter notebook and the pythn script were run both on Google colab and Nimblebox.ai cloud platforms to check the speed and reliability of the platform or the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Aider\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import objgraph -- memory allocation checks \n",
    "#import joblib -- importing the model objects to .pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read the Train data and merge it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_domain=pd.read_csv('frequency_domain_features_train.csv')\n",
    "heart_rate_non_linear=pd.read_csv('heart_rate_non_linear_features_train.csv')\n",
    "time_domain=pd.read_csv('time_domain_features_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Merging and Preprocessing\n",
    "\n",
    "1. We get the merged data from the preprocessing file \n",
    "\n",
    "2. Aider .py script was written to practise clean coding of keeping different tasks in different sheets \n",
    "\n",
    "3. Aider does the following actions under preprocessing \n",
    "\n",
    "4. Default checks for the data - shape,summary stats,null value checks ,datatype info\n",
    "5. Preprocessing - Merge dataframes and apply one hot encoding on the data ,drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Aider.preprocessing(frequency_domain,heart_rate_non_linear,time_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Very basic EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Correlations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "sns.heatmap(data.corr(),\n",
    "            annot=True,\n",
    "            linewidths=.5,\n",
    "            center=0,\n",
    "            cbar=False,\n",
    "            cmap=\"YlGnBu\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependent variable distribution "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data['HR'].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.distplot(data['HR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(data['HR'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "#data[\"HR\"] = np.log(data[\"HR\"])\n",
    "\n",
    "#Check the new distribution \n",
    "#sns.distplot(data['HR']);\n",
    "\n",
    "data['HR']=np.exp(data['HR'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#sns.distplot(data['HR'])\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(data['HR'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outcomes/Insights \n",
    "\n",
    "1. There's a few highly correlated variables in teh dataset and in hindsight dropping none of which helped improved the model,we have retained all teh features in teh model as teh MAE seemed to be dropping otherwise\n",
    "\n",
    "2. The distribution of the target variable is checked and we see that is very skewed and may need transformation and hence box-cox transformation was tried upon teh dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box Cox Transformation of the variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_t=data.drop(['KURT','SKEW','KURT_REL_RR','SKEW_REL_RR','HR'],axis=1)\n",
    "numeric_feats = data_t.dtypes[data_t.dtypes != 'object'].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = data_t[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    data_t[feat] = boxcox1p(data_t[feat], lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outcome of Box cox transformation\n",
    "\n",
    "1. All the variables in teh data seemed to be highly skewed ,a trial was ocnducted with a box cox transformation of the variables that had skewness>0.75 .\n",
    "2. The transformation of teh target variable and the other variables showed no significant uplift in the improvement of the model scores .\n",
    "3. The changes were therefore made redundant keeping in mind the 'No-impact' of the skewness changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Deleting the dataframes to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del frequency_domain\n",
    "del heart_rate_non_linear\n",
    "del time_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Libraries \n",
    "\n",
    "#Importing required libraries\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,BaggingRegressor\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Modeling Steps followed\n",
    "\n",
    "1. The Train test split and scaling code is in the Aider python script under the function 'split_train_test' \n",
    "\n",
    "2. There's a modeling helper function written to assist the process of running an algorithm clutter free . \n",
    "\n",
    "3. The function tracks the time elapsed during the run of each algorithm ,fitting the train data and predict it on the test data . The metrics printed along with train and test MAE are r2 and a model object is returned as an output of the helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multiple modeling experiments on different regressors - Checking basic models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgre=BaggingRegressor(random_state=42)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "bg_model=Aider.run_algorithm(bgre,x_train,y_train,x_test,y_test,'BaggingRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM Regressor "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lgbm=lgb.LGBMRegressor(random_state=42)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "lgbm_model=Aider.run_algorithm(lgbm,x_train,y_train,x_test,y_test,'LGBMRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost Regressor "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gbre=GradientBoostingRegressor(random_state=42)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "gbre_model=Aider.run_algorithm(gbre,x_train,y_train,x_test,y_test,'GBREMRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfre=RandomForestRegressor(random_state=42)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "rfre_model=Aider.run_algorithm(rfre,x_train,y_train,x_test,y_test,'RFRegressor')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduced learning rate to 0.1 - improved performance of MAE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "adre=AdaBoostRegressor(random_state=42,n_estimators=50,learning_rate=0.1)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "adre_model=Aider.run_algorithm(adre,x_train,y_train,x_test,y_test,'AdaBoostRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduced learning rate to 0.01 - degraded performance of MAE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "adre_1=AdaBoostRegressor(random_state=42,n_estimators=50,learning_rate=0.1)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "adre_model=Aider.run_algorithm(adre_1,x_train,y_train,x_test,y_test,'AdaBoostRegressor_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduced learning rate to 0.05 - degraded performance of MAE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adre_2=AdaBoostRegressor(random_state=42,n_estimators=50,learning_rate=0.05)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "adre_model=Aider.run_algorithm(adre_2,x_train,y_train,x_test,y_test,'AdaBoostRegressor_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model_dt=DecisionTreeRegressor(min_samples_leaf=1,max_depth=1000)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)                                                \n",
    "DT_model=Aider.run_algorithm(model_dt,x_train,y_train,x_test,y_test,'DTRegressor')\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Regressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "svr=SVR()\n",
    "run_algorithm(svr,x_train,y_train,x_test,y_test,'SupportVectorRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoks=50\n",
    "neurons=[64,32,1]\n",
    "activation=['relu']\n",
    "batch_size=[10,20,30]\n",
    "learning_rate=0.0002\n",
    "optimizer=[optimizers.Adam(learning_rate=learning_rate)\n",
    "           ]\n",
    "\n",
    "def dnn_model():\n",
    "\n",
    "    model=Sequential()\n",
    "    model.add(Dense(neurons[0], input_dim=x_train.shape[1],activation = activation[0]))\n",
    "    model.add(Dense(neurons[1], activation = activation[0]))\n",
    "    model.add(Dense(neurons[1], activation = activation[0]))\n",
    "    model.add(Dense(neurons[2], activation = activation[0]))\n",
    "\n",
    "    model.compile(optimizer=optimizer[0],loss='mse',metrics=['mae'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, Callback, History, EarlyStopping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model=dnn_model()\n",
    "\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('best_model.h5', \n",
    "                                     monitor='val_loss', \n",
    "                                     verbose=0, \n",
    "                                     save_best_only=True, \n",
    "                                     save_freq='epoch')\n",
    "\n",
    "history = model.fit(x_train,y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=20,\n",
    "                    validation_data=(x_test,y_test),\n",
    "                    callbacks=[model_checkpoint_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Features Importnaces"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "featureimportance_rf=rf_tuned.feature_importances_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# View a list of the features and their importance scores\n",
    "indices = np.argsort(featureimportance_rf)[::-1][:15]\n",
    "a = data.columns[:]\n",
    "features= a.drop('HR',1)\n",
    "#plot it\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), featureimportance_rf[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PCA "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Apply PCA on the Sampled data and Run logistic Regression\n",
    "pca = PCA(svd_solver='randomized', random_state=42)\n",
    "pca.fit(x_train)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outcomes - Checking basic Models \n",
    "\n",
    "1. There were a few Linear models with polynomial degrees tried ,none of which matched the low  MAE score of Random Forest ,Bagging Regressor AdaBoost Regressor\n",
    "\n",
    "2. Multiple base models for bagging and boosting were tried to see the outputs with low MAE's .\n",
    "\n",
    "3. Random Forest ,Baging Regressor ,Adaboost Regressor were clear winners "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etre=ExtraTreesRegressor(random_state=42,n_jobs=-1,n_estimators=100)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "etre_model=Aider.run_algorithm(etre,x_train,y_train,x_test,y_test,'ETRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Extra Trees Regressor seems like the most efficient model ,with the least MAE score so far,we will proceed with our experiments in tuning it and ensembling it as well as stacking it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etre_2=ExtraTreesRegressor(random_state=42,n_jobs=-1,n_estimators=1000)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0)\n",
    "etre_model=Aider.run_algorithm(etre_2,x_train,y_train,x_test,y_test,'ETRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the model on 100% data after a test on 80 Train and 20 Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etre_4=ExtraTreesRegressor(random_state=42,n_jobs=-1,n_estimators=1330,max_depth=50)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0)\n",
    "etre_model=Aider.run_algorithm(etre_4,x_train,y_train,x_test,y_test,'ETRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment - 3 \n",
    "\n",
    "## Checking the combination of various parameters and their effects on the model output i.e Train and Test MAE values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "etre_4=ExtraTreesRegressor(random_state=42,n_jobs=-1,n_estimators=900,max_depth=50)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "etre_model=Aider.run_algorithm(etre_4,x_train,y_train,x_test,y_test,'ETRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment - 4 \n",
    "## Can job lib pkl files store the model ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(etre_model,'etre_model_1330.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "etre_4=ExtraTreesRegressor(random_state=42,n_jobs=-1,n_estimators=1330)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.000001)\n",
    "etre_model=Aider.run_algorithm(etre_4,x_train,y_train,x_test,y_test,'ETRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ensemble -  Adaboost & Extra Trees Regressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adre=AdaBoostRegressor(etre,learning_rate=1,random_state=42,n_estimators=50)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.00005)\n",
    "ad_model=Aider.run_algorithm(adre,x_train,y_train,x_test,y_test,'AdaboostRegressor')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "adre=AdaBoostRegressor(etre,learning_rate=1,random_state=42,n_estimators=100)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.00005)\n",
    "ad_model=Aider.run_algorithm(adre,x_train,y_train,x_test,y_test,'AdaboostRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lessons learnt -  reflection \n",
    "\n",
    "1. The ensemble produces a good output however fails the Kernel/Kernel\n",
    "dies when number of estimators increase ,either in the bse estimator \n",
    "or Adaboost regressor .\n",
    "2. Model objects are too huge to  be stored in .pkl files \n",
    "3. Lack of computational efficiency rules this model out even though it produces very good results ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Stacking -  Voting Regressor + Stacking Regressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import VotingRegressor\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "estimators=[('etre',ExtraTreesRegressor(random_state=42,n_jobs=-1)),('bgre',BaggingRegressor(random_state=42)),('rf',RandomForestRegressor(random_state=42))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bgre=VotingRegressor(estimators=estimators,random_state=42,n_jobs=-1)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "vore_model=Aider.run_algorithm(vore,x_train,y_train,x_test,y_test,'ETRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Regressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "estimators=[('bgre',BaggingRegressor(random_state=42))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stack = StackingRegressor(estimators=estimators,final_estimator=RandomForestRegressor(random_state=42))\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)                                                \n",
    "stacked_model=Aider.run_algorithm(stack,x_train,y_train,x_test,y_test,'StackedRegressor')\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outcome\n",
    "\n",
    "1. The output was not satisfactory enough to consider Voting Regressor as a stacking option.\n",
    "\n",
    "2. The stacking regressor for RF and Bagging Regressor did not generate as good an output as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ensemble- Bagging+ExtraTrees Regressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bgre=BaggingRegressor(base_estimator=ExtraTreesRegressor(random_state=42),n_jobs=-1)\n",
    "x_train,x_test,y_train,y_test,sc=Aider.split_train_test(data,test_ratio=0.2)\n",
    "bgre_model=Aider.run_algorithm(bgre,x_train,y_train,x_test,y_test,'BGRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Outcome\n",
    "\n",
    "The output was not satisfactory and hence we decide to not use this ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting 'data' as we do not need it anymore and clearing some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Steps followed`\n",
    "\n",
    "1. Merge the 3 test datasets and create a single dataframe\n",
    "2. Preprocess the dataframe to keep it model ready.\n",
    "3. Predict using the model that worked best from the above trials\n",
    "4. Send the results to a dataframe and convert the dataframe to a .csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and create a test file ,by merging the .csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_domain_test=pd.read_csv('frequency_domain_features_test.csv')\n",
    "heart_rate_non_linear_test=pd.read_csv('heart_rate_non_linear_features_test.csv')\n",
    "time_domain_test=pd.read_csv('time_domain_features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=frequency_domain_test.merge(heart_rate_non_linear_test,on='uuid')\n",
    "datatest1 = temp.merge(time_domain_test,on='uuid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying preprocessing from the 'Aider' file we imported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatest=Aider.preprocessing(frequency_domain_test,heart_rate_non_linear_test,time_domain_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the dataframes to clear some memory so the kernel does not die."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del frequency_domain_test\n",
    "del heart_rate_non_linear_test\n",
    "del time_domain_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=datatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling ,predicting on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val=sc.transform(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datatest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val=etre_4.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadig the results to a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatest.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['uuid']=datatest1['uuid']\n",
    "submission_df['HR']=y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to a .csv file and saving to the current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission_ETRE-1330.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Multiple submissions were made with the outputs for Randomforest regressor ,ensemble models a)Adaboost Regressor + Random Forest b)AdaBoost Regressor + ExtraTrees Regressor c)Bagging Regressor (tuned)  \n",
    "\n",
    "2.Extratrees Regressor was tried with multiple tuning parameters and a tuned model of 1330 estimators produced the least MAE ,compared to all other stacked and ensemble models .There can be an ensemble of Adaboost and ExtraTrees which could work very well ,but the computational efficiency with the Kernel dying and memory errors throwing,the least score we could get to was 0.0014xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## End ############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
